{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# IaC-GPT Training on Kaggle (v3.1)\n\nTrain a domain-specific Infrastructure-as-Code LLM for free on Kaggle GPUs.\n\n**What's new in v3.1:**\n- **Fixed T4 GPU hang**: Proper BF16 tensor core detection (2-5min compilation, not 27+min)\n- Auto-detects compute capability to use FP16 on T4, BF16 on Ampere+\n- No more \"Tesla T4 does not support bfloat16 compilation natively\" warnings\n\n**v3 features:**\n- Checkpoint persistence: auto-save to Kaggle Datasets\n- Training data caching: never re-scrape\n- Resume training: pick up exactly where you left off\n- 4-tier data curriculum + secret sanitization\n\n## Setup:\n1. Enable GPU: **Settings → Accelerator → GPU P100**\n2. Enable Internet: **Settings → Internet → On**\n3. Add your Kaggle username in Cell 1\n4. Run all cells\n\n**Estimated time:** d12 ~6-8hrs on P100 (single GPU)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# === CONFIGURATION ===\n# Model depth should match corpus size to avoid overfitting:\n#   d12 (~115M params) for < 100M tokens  <-- recommended for current IaC corpus\n#   d16 (~400M params) for 100-500M tokens\n#   d24 (~1.6B params) for 500M+ tokens\n\nMODEL_DEPTH = 12        # 12 recommended for current corpus size\nBATCH_SIZE = 2          # 2 for P100 GPU (prevents OOM)\nNUM_GPUS = 1            # Kaggle P100 (single GPU)\nWINDOW_PATTERN = \"L\"    # Full attention (don't use SSSL on single GPU)\nDATA_RATIO = 8          # target-param-data-ratio (5=aggressive, 8=balanced, 10=conservative)\n\n# === RESUME CONFIGURATION ===\n# Set RESUME = True if continuing a previous training run.\n# Set RESUME_STEP to the step number to resume from, or -1 to auto-detect the latest.\nRESUME = False           # Set True to resume from a previous session\nRESUME_STEP = -1         # -1 = auto-detect last checkpoint, or set a specific step number\n\n# === KAGGLE IDENTITY ===\n# Your Kaggle username (for saving datasets). Find it at kaggle.com -> Account.\nKAGGLE_USERNAME = \"YOUR_KAGGLE_USERNAME\"  # <-- SET YOUR KAGGLE USERNAME HERE\n\n# Dataset slugs (auto-generated, don't change)\nCHECKPOINT_DATASET = f\"iac-gpt-checkpoints-d{MODEL_DEPTH}\"\nTRAINING_DATA_DATASET = \"iac-gpt-training-data\"\n\nprint(f\"Training config: d{MODEL_DEPTH} model, batch_size={BATCH_SIZE}, gpus={NUM_GPUS}, ratio={DATA_RATIO}\")\nprint(f\"Resume: {RESUME} (step={RESUME_STEP})\")\nprint(f\"Kaggle user: {KAGGLE_USERNAME}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone or update the repo\n",
    "!git clone https://github.com/holynakamoto/iacgpt.git nanochat 2>/dev/null || \\\n",
    "    (cd nanochat && git pull origin master)\n",
    "%cd nanochat\n",
    "\n",
    "# Verify we have the T4 fix\n",
    "!git log --oneline -1\n",
    "print(\"\\n✓ Expected: e9a4559 Fix T4 BF16 detection: Use compute capability...\")\n",
    "\n",
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies\n!pip install -q tiktoken pyarrow filelock rustbpe wandb tabulate regex zstandard pyyaml\n\n# Install flash-attn (optional, may fail on older GPUs)\n!pip install -q flash-attn --no-build-isolation 2>/dev/null || echo \"Flash attention not available (OK for P100)\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify BF16 Detection\nimport sys\nsys.path.insert(0, '/kaggle/working/nanochat')\nimport torch\nfrom common import has_bf16_support\n\nprint(\"=\" * 70)\nprint(\"GPU BF16 DETECTION TEST\")\nprint(\"=\" * 70)\nprint(f\"GPU: {torch.cuda.get_device_name(0)}\")\nmajor, minor = torch.cuda.get_device_capability(0)\nprint(f\"Compute capability: {major}.{minor}\")\nprint(f\"\\ntorch.cuda.is_bf16_supported(): {torch.cuda.is_bf16_supported()} ← buggy (detects emulation)\")\nprint(f\"has_bf16_support(): {has_bf16_support()} ← correct (checks tensor cores)\\n\")\n\nif has_bf16_support():\n    print(\"✓ GPU has BF16 tensor cores -> will use BF16\")\nelse:\n    print(\"✓ GPU lacks BF16 tensor cores -> will use FP16 (native tensor cores)\")\n    print(\"  (P100 uses FP16, T4 also uses FP16)\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collect & Prepare IaC Training Data\n",
    "\n",
    "The data pipeline:\n",
    "1. **Scrape** 25 curated IaC repos (Terraform, K8s, Ansible, Crossplane, Docker)\n",
    "2. **Sanitize** to remove secrets (AWS keys, SSH keys, API tokens, real IPs)\n",
    "3. **Extract** Requirement→Code pairs for Tier 2 curriculum\n",
    "4. **Repackage** into parquet shards for the dataloader\n",
    "\n",
    "**If resuming:** This section checks for a cached training data dataset first.\n",
    "If found, it skips the 15-minute scrape entirely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, subprocess\n",
    "\n",
    "CACHE_DIR = os.path.expanduser(\"~/.cache/nanochat\")\n",
    "DATA_DIR = os.path.join(CACHE_DIR, \"iac_data\")\n",
    "BASE_DATA = os.path.join(CACHE_DIR, \"base_data\")\n",
    "\n",
    "# Check if cached training data exists (from a previous session's Kaggle Dataset)\n",
    "cached_data_path = f\"/kaggle/input/{TRAINING_DATA_DATASET}\"\n",
    "has_cached_data = os.path.isdir(cached_data_path) and glob.glob(f\"{cached_data_path}/*.parquet\")\n",
    "\n",
    "if has_cached_data:\n",
    "    print(f\"Found cached training data at {cached_data_path}\")\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    for f in glob.glob(f\"{cached_data_path}/*\"):\n",
    "        dest = os.path.join(DATA_DIR, os.path.basename(f))\n",
    "        if not os.path.exists(dest):\n",
    "            os.symlink(f, dest)\n",
    "    # Link base_data\n",
    "    if os.path.islink(BASE_DATA):\n",
    "        os.unlink(BASE_DATA)\n",
    "    os.symlink(DATA_DIR, BASE_DATA)\n",
    "    parquets = glob.glob(f\"{BASE_DATA}/*.parquet\")\n",
    "    print(f\"Loaded {len(parquets)} cached shard(s) - skipping scrape!\")\n",
    "else:\n",
    "    print(\"No cached data found. Running full data pipeline...\")\n",
    "    # Step 1: Scrape IaC repositories (~10-15 min)\n",
    "    subprocess.run([\"bash\", \"dev/fast_scrape_iac.sh\"], input=b\"n\", check=True)\n",
    "\n",
    "    # Step 2: Sanitize - remove secrets before training\n",
    "    print(\"\\nSanitizing data...\")\n",
    "    subprocess.run([\"python3\", \"dev/sanitize_iac.py\", \"--raw-dir\", \"data/iac_raw_cloned\", \"--dry-run\"], check=False)\n",
    "    subprocess.run([\"python3\", \"dev/sanitize_iac.py\", \"--raw-dir\", \"data/iac_raw_cloned\"], check=False)\n",
    "\n",
    "    # Step 3: Convert to training shards\n",
    "    print(\"\\nCreating training shards...\")\n",
    "    subprocess.run([\n",
    "        \"python3\", \"dev/repackage_iac_data.py\",\n",
    "        \"--input-dir\", \"data/iac_raw_cloned\",\n",
    "        \"--output-dir\", DATA_DIR,\n",
    "        \"--include-synthetic\", \"--include-docs\"\n",
    "    ], check=True)\n",
    "\n",
    "    # Ensure at least 2 shards for distributed training\n",
    "    shards = sorted(glob.glob(f\"{DATA_DIR}/shard_*.parquet\"))\n",
    "    if len(shards) == 1:\n",
    "        import shutil\n",
    "        shutil.copy2(shards[0], os.path.join(DATA_DIR, \"shard_00001.parquet\"))\n",
    "\n",
    "    # Link base_data\n",
    "    if os.path.islink(BASE_DATA):\n",
    "        os.unlink(BASE_DATA)\n",
    "    os.symlink(DATA_DIR, BASE_DATA)\n",
    "\n",
    "parquets = sorted(glob.glob(f\"{BASE_DATA}/*.parquet\"))\n",
    "total_mb = sum(os.path.getsize(p) for p in parquets) / 1e6\n",
    "print(f\"\\nTraining data ready: {len(parquets)} shards, {total_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE tokenizer on IaC data (~1 min)\n",
    "!python3 -m scripts.tok_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tokenizer compression\n",
    "!python3 -m scripts.tok_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train IaC-GPT Base Model\n",
    "\n",
    "If `RESUME = True`, the training command will load the checkpoint from a previous session\n",
    "and continue from where it left off. The optimizer state, dataloader position, and loss\n",
    "tracking are all restored automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, shutil\n",
    "\n",
    "CKPT_DIR = os.path.expanduser(f\"~/.cache/nanochat/base_checkpoints/iac-gpt-d{MODEL_DEPTH}\")\n",
    "resume_step = -1\n",
    "\n",
    "if RESUME:\n",
    "    # Look for checkpoint dataset from previous session\n",
    "    ckpt_input = f\"/kaggle/input/{CHECKPOINT_DATASET}\"\n",
    "    if os.path.isdir(ckpt_input) and glob.glob(f\"{ckpt_input}/model_*.pt\"):\n",
    "        print(f\"Restoring checkpoints from {ckpt_input}...\")\n",
    "        os.makedirs(CKPT_DIR, exist_ok=True)\n",
    "        for f in glob.glob(f\"{ckpt_input}/*\"):\n",
    "            dest = os.path.join(CKPT_DIR, os.path.basename(f))\n",
    "            shutil.copy2(f, dest)\n",
    "\n",
    "        # Find the latest step\n",
    "        model_files = sorted(glob.glob(f\"{CKPT_DIR}/model_*.pt\"))\n",
    "        if model_files:\n",
    "            latest = model_files[-1]\n",
    "            resume_step = int(os.path.basename(latest).replace(\"model_\", \"\").replace(\".pt\", \"\"))\n",
    "            print(f\"Found checkpoint at step {resume_step}\")\n",
    "        else:\n",
    "            print(\"WARNING: No model files found in checkpoint dataset!\")\n",
    "            resume_step = -1\n",
    "    else:\n",
    "        print(f\"WARNING: RESUME=True but no checkpoint dataset found at {ckpt_input}\")\n",
    "        print(f\"Add your '{CHECKPOINT_DATASET}' dataset as an Input to this notebook.\")\n",
    "        resume_step = -1\n",
    "\n",
    "    # Allow manual override\n",
    "    if RESUME_STEP > 0:\n",
    "        resume_step = RESUME_STEP\n",
    "        print(f\"Using manually specified resume step: {resume_step}\")\n",
    "\n",
    "    if resume_step > 0:\n",
    "        print(f\"\\nWill resume training from step {resume_step}\")\n",
    "    else:\n",
    "        print(\"\\nNo checkpoint to resume from. Starting fresh.\")\n",
    "else:\n",
    "    print(\"Fresh training run (RESUME=False)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Train IaC-GPT model\nimport os\nos.environ['OMP_NUM_THREADS'] = '1'\nos.environ['PYTHONUNBUFFERED'] = '1'\n\ncmd = f\"\"\"torchrun --standalone --nproc_per_node={NUM_GPUS} -m scripts.base_train -- \\\n    --depth={MODEL_DEPTH} \\\n    --device-batch-size={BATCH_SIZE} \\\n    --window-pattern={WINDOW_PATTERN} \\\n    --target-param-data-ratio={DATA_RATIO} \\\n    --run=dummy \\\n    --model-tag=iac-gpt-d{MODEL_DEPTH} \\\n    --eval-every=100 \\\n    --sample-every=100 \\\n    --save-every=100\"\"\"\n\n# Add resume flag if we have a checkpoint\nif resume_step > 0:\n    cmd += f\" \\\\\\n    --resume-from-step={resume_step}\"\n    print(f\"RESUMING from step {resume_step}\")\n\nprint(\"=\" * 80)\nprint(\"EXPECTED OUTPUT:\")\nprint(\"  GPU: Tesla P100-PCIE-16GB | Peak FLOPS (FP16): ~1.9e+13\")\nprint(\"  GPU does not support bfloat16, using float16 with GradScaler\")\nprint(\"  Compilation: 2-5 minutes\")\nprint(\"  Training loop starts showing loss values\")\nprint(\"=\" * 80)\nprint(f\"\\nCommand: {cmd}\\n\")\n\n!{cmd}"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4b. Save Checkpoints & Data to Kaggle Datasets\n",
    "\n",
    "Run this cell after training completes (or if you need to stop early).\n",
    "It uploads your checkpoints and training data as private Kaggle Datasets\n",
    "so you can resume in a new session.\n",
    "\n",
    "**To resume later:**\n",
    "1. Start a new notebook session\n",
    "2. Add these datasets as Inputs: `iac-gpt-checkpoints-d{MODEL_DEPTH}` and `iac-gpt-training-data`\n",
    "3. Set `RESUME = True` in the config cell\n",
    "4. Run all cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os, glob, shutil\n",
    "\n",
    "def save_to_kaggle_dataset(source_dir, dataset_slug, username, file_patterns=(\"*\",)):\n",
    "    \"\"\"Upload a directory's contents as a Kaggle Dataset.\"\"\"\n",
    "    staging = f\"/kaggle/working/_staging_{dataset_slug}\"\n",
    "    if os.path.exists(staging):\n",
    "        shutil.rmtree(staging)\n",
    "    os.makedirs(staging)\n",
    "\n",
    "    # Copy matching files to staging\n",
    "    count = 0\n",
    "    for pattern in file_patterns:\n",
    "        for f in glob.glob(os.path.join(source_dir, pattern)):\n",
    "            if os.path.isfile(f):\n",
    "                shutil.copy2(f, staging)\n",
    "                count += 1\n",
    "\n",
    "    if count == 0:\n",
    "        print(f\"WARNING: No files matched in {source_dir}\")\n",
    "        return False\n",
    "\n",
    "    # Create dataset-metadata.json\n",
    "    meta = {\n",
    "        \"title\": dataset_slug,\n",
    "        \"id\": f\"{username}/{dataset_slug}\",\n",
    "        \"licenses\": [{\"name\": \"CC0-1.0\"}]\n",
    "    }\n",
    "    with open(os.path.join(staging, \"dataset-metadata.json\"), \"w\") as f:\n",
    "        json.dump(meta, f)\n",
    "\n",
    "    # Try to create (first time) or update (subsequent times)\n",
    "    import subprocess\n",
    "    result = subprocess.run(\n",
    "        [\"kaggle\", \"datasets\", \"create\", \"-p\", staging, \"--dir-mode\", \"zip\"],\n",
    "        capture_output=True, text=True\n",
    "    )\n",
    "    if \"already exists\" in result.stderr.lower() or result.returncode != 0:\n",
    "        result = subprocess.run(\n",
    "            [\"kaggle\", \"datasets\", \"version\", \"-p\", staging, \"-m\", \"auto-save\", \"--dir-mode\", \"zip\"],\n",
    "            capture_output=True, text=True\n",
    "        )\n",
    "\n",
    "    shutil.rmtree(staging)\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(f\"Saved {count} files to kaggle.com/{username}/{dataset_slug}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(f\"ERROR saving dataset: {result.stderr}\")\n",
    "        return False\n",
    "\n",
    "# === Save checkpoints ===\n",
    "ckpt_dir = os.path.expanduser(f\"~/.cache/nanochat/base_checkpoints/iac-gpt-d{MODEL_DEPTH}\")\n",
    "if os.path.isdir(ckpt_dir) and glob.glob(f\"{ckpt_dir}/model_*.pt\"):\n",
    "    print(\"Saving checkpoints...\")\n",
    "    save_to_kaggle_dataset(\n",
    "        ckpt_dir, CHECKPOINT_DATASET, KAGGLE_USERNAME,\n",
    "        file_patterns=(\"model_*.pt\", \"meta_*.json\", \"optim_*.pt\")\n",
    "    )\n",
    "else:\n",
    "    print(f\"No checkpoints found at {ckpt_dir}\")\n",
    "\n",
    "# === Save training data (only on first run) ===\n",
    "data_dir = os.path.expanduser(\"~/.cache/nanochat/iac_data\")\n",
    "if not has_cached_data and os.path.isdir(data_dir):\n",
    "    print(\"\\nSaving training data for future sessions...\")\n",
    "    save_to_kaggle_dataset(\n",
    "        data_dir, TRAINING_DATA_DATASET, KAGGLE_USERNAME,\n",
    "        file_patterns=(\"*.parquet\",)\n",
    "    )\n",
    "else:\n",
    "    print(\"\\nTraining data already cached as dataset (skipping upload)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model\n",
    "cmd = f\"torchrun --standalone --nproc_per_node={NUM_GPUS} -m scripts.base_eval -- --device-batch-size={BATCH_SIZE}\"\n",
    "print(f\"Running: {cmd}\")\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test\n",
    "!python3 -m scripts.chat_cli \\\n",
    "    -i base \\\n",
    "    -p \"Write a Terraform module for an EKS cluster\" \\\n",
    "    -t 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress model for download\n",
    "!tar -czf iac_gpt_model.tar.gz ~/.cache/nanochat/base_checkpoints/\n",
    "!tar -czf iac_gpt_tokenizer.tar.gz ~/.cache/nanochat/tokenizer/\n",
    "\n",
    "print(\"\\n✅ Model files ready for download:\")\n",
    "!ls -lh *.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download via Kaggle Output panel\n",
    "from IPython.display import FileLink\n",
    "print(\"Download your trained model:\")\n",
    "display(FileLink('iac_gpt_model.tar.gz'))\n",
    "display(FileLink('iac_gpt_tokenizer.tar.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Training Complete!\n\nYour IaC-GPT model is ready.\n\n### What's fixed in v3.1:\n✅ **GPU compatibility**: Proper compute capability detection for P100/T4\n- P100 (6.0) and T4 (7.5) both use FP16 tensor cores (fast and stable)\n- Compilation: 2-5 minutes\n- No more \"does not support bfloat16\" warnings\n\n### Resume a killed session:\n1. Start a **new** Kaggle notebook\n2. Go to **Add Data** (right sidebar) and add your saved datasets:\n   - `iac-gpt-checkpoints-d12` (your model checkpoints)\n   - `iac-gpt-training-data` (your parquet shards)\n3. Set `RESUME = True` in the config cell\n4. Run all cells -- scraping is skipped, training picks up from last checkpoint\n\n### Download & use locally:\n```bash\ntar -xzf iac_gpt_model.tar.gz\npython3 -m scripts.chat_cli -i base -p \"Create a Terraform module for an EKS cluster\"\n```\n\n### Try prompts:\n- \"Create a Terraform module for an EKS cluster\"\n- \"Write a Kubernetes deployment for nginx\"\n- \"Generate an Ansible playbook to deploy a web app\"\n\n---\n\n**Model Stats:**\n- Parameters: ~286M (d12)\n- Training Data: 11,188 IaC files\n- Training Time: ~6-8 hours on P100 (single GPU)\n- Cost: **FREE on Kaggle!**"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}