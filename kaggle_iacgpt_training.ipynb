{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IaC-GPT Training on Kaggle\n",
    "\n",
    "Train a domain-specific Infrastructure-as-Code LLM for free on Kaggle GPUs.\n",
    "\n",
    "## Setup:\n",
    "1. Enable GPU: **Settings â†’ Accelerator â†’ GPU T4 x2**\n",
    "2. Enable Internet: **Settings â†’ Internet â†’ On**\n",
    "3. Run all cells\n",
    "\n",
    "**Estimated time:** d12 ~3hrs, d16 ~8hrs, d20 ~18hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "# Change these settings based on your available Kaggle hours\n",
    "\n",
    "MODEL_DEPTH = 12        # 12 (~300M, ~3hrs) | 16 (~500M, ~8hrs) | 20 (~800M, ~18hrs)\n",
    "BATCH_SIZE = 4          # 4 for T4 GPUs (DO NOT use 8 - will OOM!)\n",
    "NUM_GPUS = 2            # Kaggle T4 x2\n",
    "WINDOW_PATTERN = \"L\"    # Full attention (don't use SSSL on T4s)\n",
    "\n",
    "print(f\"Training config: d{MODEL_DEPTH} model, batch_size={BATCH_SIZE}, gpus={NUM_GPUS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo\n",
    "!git clone https://github.com/holynakamoto/nanochat.git\n",
    "%cd nanochat\n",
    "\n",
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tiktoken pyarrow filelock rustbpe wandb tabulate regex zstandard\n",
    "\n",
    "# Install flash-attn (optional, may fail)\n",
    "!pip install -q flash-attn --no-build-isolation 2>/dev/null || echo \"Flash attention not available\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collect IaC Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape IaC repositories (takes ~10-15 min)\n",
    "!bash dev/fast_scrape_iac.sh <<< 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to training shards\n",
    "!python3 dev/repackage_iac_data.py \\\n",
    "    --input-dir data/iac_raw_cloned \\\n",
    "    --output-dir ~/.cache/nanochat/iac_data \\\n",
    "    --include-synthetic \\\n",
    "    --include-docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data directories\n",
    "!cp ~/.cache/nanochat/iac_data/shard_00000.parquet ~/.cache/nanochat/iac_data/shard_00001.parquet\n",
    "!ln -sf ~/.cache/nanochat/iac_data ~/.cache/nanochat/base_data\n",
    "!ls -la ~/.cache/nanochat/base_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE tokenizer on IaC data (~1 min)\n",
    "!python3 -m scripts.tok_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tokenizer compression\n",
    "!python3 -m scripts.tok_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train IaC-GPT Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train IaC-GPT model\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "cmd = f\"\"\"torchrun --standalone --nproc_per_node={NUM_GPUS} -m scripts.base_train -- \\\n",
    "    --depth={MODEL_DEPTH} \\\n",
    "    --device-batch-size={BATCH_SIZE} \\\n",
    "    --window-pattern={WINDOW_PATTERN} \\\n",
    "    --target-param-data-ratio=5 \\\n",
    "    --run=dummy \\\n",
    "    --model-tag=iac-gpt-d{MODEL_DEPTH} \\\n",
    "    --eval-every=200 \\\n",
    "    --sample-every=500 \\\n",
    "    --save-every=1000\"\"\"\n",
    "\n",
    "print(f\"Running: {cmd}\")\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model\n",
    "cmd = f\"torchrun --standalone --nproc_per_node={NUM_GPUS} -m scripts.base_eval -- --device-batch-size={BATCH_SIZE}\"\n",
    "print(f\"Running: {cmd}\")\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick inference test\n",
    "!python3 -m scripts.chat_cli \\\n",
    "    --model ~/.cache/nanochat/base_checkpoints/iac-gpt-d{MODEL_DEPTH}/latest_checkpoint \\\n",
    "    --max-new-tokens 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Download Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compress model for download\n",
    "!tar -czf iac_gpt_model.tar.gz ~/.cache/nanochat/base_checkpoints/\n",
    "!tar -czf iac_gpt_tokenizer.tar.gz ~/.cache/nanochat/tokenizer/\n",
    "\n",
    "print(\"\\nâœ… Model files ready for download:\")\n",
    "!ls -lh *.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download via Kaggle Output panel\n",
    "from IPython.display import FileLink\n",
    "print(\"Download your trained model:\")\n",
    "display(FileLink('iac_gpt_model.tar.gz'))\n",
    "display(FileLink('iac_gpt_tokenizer.tar.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ‰ Training Complete!\n",
    "\n",
    "Your IaC-GPT model is ready!\n",
    "\n",
    "### Next Steps:\n",
    "1. Download the model files above\n",
    "2. Extract locally: `tar -xzf iac_gpt_model.tar.gz`\n",
    "3. Run inference: `python3 -m scripts.chat_cli --model ~/.cache/nanochat/base_checkpoints/iac-gpt-d12/latest_checkpoint`\n",
    "\n",
    "### Try it out:\n",
    "- \"Create a Terraform module for an EKS cluster\"\n",
    "- \"Write a Kubernetes deployment for nginx\"\n",
    "- \"Generate an Ansible playbook to deploy a web app\"\n",
    "\n",
    "---\n",
    "\n",
    "**Model Stats:**\n",
    "- Parameters: ~286M (d12)\n",
    "- Training Data: 11,188 IaC files\n",
    "- Training Time: ~3 hours\n",
    "- Cost: **FREE on Kaggle!**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
