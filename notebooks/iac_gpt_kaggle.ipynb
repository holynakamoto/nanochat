{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IaC-GPT Training on Kaggle\n",
    "\n",
    "Train a ~800M parameter Infrastructure-as-Code specialist LLM for free on Kaggle GPUs.\n",
    "\n",
    "**Setup:**\n",
    "1. Enable GPU: Settings → Accelerator → GPU T4 x2\n",
    "2. Enable Internet: Settings → Internet → On\n",
    "3. Run all cells\n",
    "\n",
    "**Estimated time:** 15-20 hours for d20 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo\n",
    "!git clone https://github.com/holynakamoto/iacgpt.git\n",
    "%cd iacgpt\n",
    "\n",
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies (Kaggle already has most, just need a few extras)\n!pip install -q tiktoken pyarrow filelock rustbpe wandb tabulate regex zstandard\n\n# Install flash-attn for faster training (optional, may fail on some systems)\n!pip install -q flash-attn --no-build-isolation 2>/dev/null || echo \"Flash attention not available, using SDPA fallback\""
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collect IaC Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape IaC repositories (takes ~10-15 min)\n",
    "!bash dev/fast_scrape_iac.sh <<< 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to training shards\n",
    "!python dev/repackage_iac_data.py \\\n",
    "    --input-dir data/iac_raw_cloned \\\n",
    "    --output-dir ~/.cache/nanochat/iac_data \\\n",
    "    --include-synthetic --include-docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data directories\n",
    "!cp ~/.cache/nanochat/iac_data/shard_00000.parquet ~/.cache/nanochat/iac_data/shard_00001.parquet\n",
    "!ln -sf ~/.cache/nanochat/iac_data ~/.cache/nanochat/base_data\n",
    "!ls -la ~/.cache/nanochat/base_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE tokenizer on IaC data (~1 min)\n",
    "!python -m scripts.tok_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tokenizer compression\n",
    "!python -m scripts.tok_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train IaC-GPT Base Model\n",
    "\n",
    "**Model options:**\n",
    "- `--depth=12` (~300M params, ~3 hrs) - Quick test\n",
    "- `--depth=16` (~500M params, ~8 hrs) - Good balance  \n",
    "- `--depth=20` (~800M params, ~18 hrs) - Best quality\n",
    "\n",
    "Adjust based on your available Kaggle hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train d20 model (~800M params)\n",
    "# For 2x T4 GPUs, use torchrun\n",
    "# Estimated time: 15-20 hours\n",
    "\n",
    "!OMP_NUM_THREADS=1 torchrun --standalone --nproc_per_node=2 -m scripts.base_train -- \\\n",
    "    --depth=20 \\\n",
    "    --device-batch-size=8 \\\n",
    "    --target-param-data-ratio=5 \\\n",
    "    --run=dummy \\\n",
    "    --model-tag=iac-gpt-d20 \\\n",
    "    --eval-every=500 \\\n",
    "    --sample-every=1000 \\\n",
    "    --save-every=2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Train d16 model (~500M params) - faster, ~8 hours\n",
    "# Uncomment to use instead of d20\n",
    "\n",
    "# !OMP_NUM_THREADS=1 torchrun --standalone --nproc_per_node=2 -m scripts.base_train -- \\\n",
    "#     --depth=16 \\\n",
    "#     --device-batch-size=8 \\\n",
    "#     --target-param-data-ratio=5 \\\n",
    "#     --run=dummy \\\n",
    "#     --model-tag=iac-gpt-d16 \\\n",
    "#     --eval-every=500 \\\n",
    "#     --sample-every=1000 \\\n",
    "#     --save-every=2000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --standalone --nproc_per_node=2 -m scripts.base_eval -- --device-batch-size=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SFT: Add DevOps Architect Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate IaC-specific identity conversations\n",
    "!python -c \"\n",
    "import json\n",
    "import os\n",
    "\n",
    "conversations = [\n",
    "    {'messages': [\n",
    "        {'role': 'user', 'content': 'Who are you?'},\n",
    "        {'role': 'assistant', 'content': 'I am IaC-GPT, a specialized AI assistant for Infrastructure-as-Code. I can help you write Terraform modules, Kubernetes manifests, Ansible playbooks, and Crossplane compositions.'}\n",
    "    ]},\n",
    "    {'messages': [\n",
    "        {'role': 'user', 'content': 'Write a Terraform module for an AWS VPC'},\n",
    "        {'role': 'assistant', 'content': '''Here is a Terraform module for an AWS VPC:\\n\\n```hcl\\nvariable \\\"vpc_cidr\\\" {\\n  default = \\\"10.0.0.0/16\\\"\\n}\\n\\nresource \\\"aws_vpc\\\" \\\"main\\\" {\\n  cidr_block = var.vpc_cidr\\n  enable_dns_hostnames = true\\n  tags = { Name = \\\"main-vpc\\\" }\\n}\\n```'''}\n",
    "    ]},\n",
    "    {'messages': [\n",
    "        {'role': 'user', 'content': 'Create a Kubernetes deployment'},\n",
    "        {'role': 'assistant', 'content': '''Here is a Kubernetes Deployment:\\n\\n```yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: app\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: app\\n  template:\\n    metadata:\\n      labels:\\n        app: app\\n    spec:\\n      containers:\\n      - name: app\\n        image: nginx:alpine\\n        ports:\\n        - containerPort: 80\\n```'''}\n",
    "    ]},\n",
    "]\n",
    "\n",
    "os.makedirs(os.path.expanduser('~/.cache/nanochat'), exist_ok=True)\n",
    "with open(os.path.expanduser('~/.cache/nanochat/iac_identity_conversations.jsonl'), 'w') as f:\n",
    "    for _ in range(50):  # Repeat for more training signal\n",
    "        for conv in conversations:\n",
    "            f.write(json.dumps(conv) + '\\n')\n",
    "print('Generated identity conversations')\n",
    "\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SFT\n",
    "!torchrun --standalone --nproc_per_node=2 -m scripts.chat_sft -- --device-batch-size=8 --run=dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with IaC prompts\n",
    "!python -m scripts.chat_cli -p \"Write a Terraform module for an EKS cluster with 3 node groups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.chat_cli -p \"Create a Kubernetes deployment with resource limits and health checks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.chat_cli -p \"Review this for security issues: resource aws_s3_bucket data { acl = public-read }\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Model Checkpoint\n",
    "\n",
    "Save your trained model before the Kaggle session ends!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model checkpoint\n",
    "!zip -r iac-gpt-model.zip ~/.cache/nanochat/checkpoints/ ~/.cache/nanochat/tokenizer/\n",
    "\n",
    "# Download from Kaggle output (check Output tab after running)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Or upload to HuggingFace Hub (optional)\n",
    "# !pip install huggingface_hub\n",
    "# from huggingface_hub import notebook_login\n",
    "# notebook_login()\n",
    "# !huggingface-cli upload holynakamoto/iac-gpt ~/.cache/nanochat/checkpoints/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}