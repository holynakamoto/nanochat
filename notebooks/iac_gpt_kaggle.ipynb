{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IaC-GPT Training on Kaggle\n",
    "\n",
    "Train a domain-specific Infrastructure-as-Code LLM for free on Kaggle GPUs.\n",
    "\n",
    "**Setup:**\n",
    "1. Enable GPU: Settings → Accelerator → GPU T4 x2\n",
    "2. Enable Internet: Settings → Internet → On\n",
    "3. Run all cells\n",
    "\n",
    "**Estimated time:** d12 ~3hrs, d16 ~8hrs, d20 ~18hrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === CONFIGURATION ===\n",
    "# Change these settings based on your available Kaggle hours\n",
    "\n",
    "MODEL_DEPTH = 12        # 12 (~300M, ~3hrs) | 16 (~500M, ~8hrs) | 20 (~800M, ~18hrs)\n",
    "BATCH_SIZE = 8          # Reduce to 4 if you get OOM errors\n",
    "NUM_GPUS = 2            # Kaggle T4 x2\n",
    "\n",
    "print(f\"Training config: d{MODEL_DEPTH} model, batch_size={BATCH_SIZE}, gpus={NUM_GPUS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repo\n",
    "!git clone https://github.com/holynakamoto/iacgpt.git\n",
    "%cd iacgpt\n",
    "\n",
    "# Check GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q tiktoken pyarrow filelock rustbpe wandb tabulate regex zstandard\n",
    "\n",
    "# Install flash-attn (optional, may fail)\n",
    "!pip install -q flash-attn --no-build-isolation 2>/dev/null || echo \"Flash attention not available\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Collect IaC Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrape IaC repositories (takes ~10-15 min)\n",
    "!bash dev/fast_scrape_iac.sh <<< 'n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to training shards\n",
    "!python dev/repackage_iac_data.py \\\n",
    "    --input-dir data/iac_raw_cloned \\\n",
    "    --output-dir ~/.cache/nanochat/iac_data \\\n",
    "    --include-synthetic --include-docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup data directories\n",
    "!cp ~/.cache/nanochat/iac_data/shard_00000.parquet ~/.cache/nanochat/iac_data/shard_00001.parquet\n",
    "!ln -sf ~/.cache/nanochat/iac_data ~/.cache/nanochat/base_data\n",
    "!ls -la ~/.cache/nanochat/base_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Custom Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE tokenizer on IaC data (~1 min)\n",
    "!python -m scripts.tok_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate tokenizer compression\n",
    "!python -m scripts.tok_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train IaC-GPT Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train IaC-GPT model\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "cmd = f\"\"\"torchrun --standalone --nproc_per_node={NUM_GPUS} -m scripts.base_train -- \\\n",
    "    --depth={MODEL_DEPTH} \\\n",
    "    --device-batch-size={BATCH_SIZE} \\\n",
    "    --target-param-data-ratio=5 \\\n",
    "    --run=dummy \\\n",
    "    --model-tag=iac-gpt-d{MODEL_DEPTH} \\\n",
    "    --eval-every=200 \\\n",
    "    --sample-every=500 \\\n",
    "    --save-every=1000\"\"\"\n",
    "\n",
    "print(f\"Running: {cmd}\")\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluate Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate base model\n",
    "cmd = f\"torchrun --standalone --nproc_per_node={NUM_GPUS} -m scripts.base_eval -- --device-batch-size={BATCH_SIZE}\"\n",
    "print(f\"Running: {cmd}\")\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. SFT: Add DevOps Architect Persona"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate IaC-specific identity conversations\n",
    "import json\n",
    "import os\n",
    "\n",
    "conversations = [\n",
    "    {'messages': [\n",
    "        {'role': 'user', 'content': 'Who are you?'},\n",
    "        {'role': 'assistant', 'content': 'I am IaC-GPT, a specialized AI assistant for Infrastructure-as-Code. I can help you write Terraform modules, Kubernetes manifests, Ansible playbooks, and Crossplane compositions.'}\n",
    "    ]},\n",
    "    {'messages': [\n",
    "        {'role': 'user', 'content': 'Write a Terraform module for an AWS VPC'},\n",
    "        {'role': 'assistant', 'content': 'Here is a Terraform module for an AWS VPC:\\n\\n```hcl\\nvariable \"vpc_cidr\" {\\n  default = \"10.0.0.0/16\"\\n}\\n\\nresource \"aws_vpc\" \"main\" {\\n  cidr_block = var.vpc_cidr\\n  enable_dns_hostnames = true\\n  tags = { Name = \"main-vpc\" }\\n}\\n```'}\n",
    "    ]},\n",
    "    {'messages': [\n",
    "        {'role': 'user', 'content': 'Create a Kubernetes deployment'},\n",
    "        {'role': 'assistant', 'content': 'Here is a Kubernetes Deployment:\\n\\n```yaml\\napiVersion: apps/v1\\nkind: Deployment\\nmetadata:\\n  name: app\\nspec:\\n  replicas: 3\\n  selector:\\n    matchLabels:\\n      app: app\\n  template:\\n    metadata:\\n      labels:\\n        app: app\\n    spec:\\n      containers:\\n      - name: app\\n        image: nginx:alpine\\n        ports:\\n        - containerPort: 80\\n```'}\n",
    "    ]},\n",
    "]\n",
    "\n",
    "os.makedirs(os.path.expanduser('~/.cache/nanochat'), exist_ok=True)\n",
    "with open(os.path.expanduser('~/.cache/nanochat/iac_identity_conversations.jsonl'), 'w') as f:\n",
    "    for _ in range(50):\n",
    "        for conv in conversations:\n",
    "            f.write(json.dumps(conv) + '\\n')\n",
    "print('Generated identity conversations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run SFT\n",
    "cmd = f\"torchrun --standalone --nproc_per_node={NUM_GPUS} -m scripts.chat_sft -- --device-batch-size={BATCH_SIZE} --run=dummy\"\n",
    "print(f\"Running: {cmd}\")\n",
    "!{cmd}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Test Your Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.chat_cli -p \"Write a Terraform module for an EKS cluster with 3 node groups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.chat_cli -p \"Create a Kubernetes deployment with resource limits and health checks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m scripts.chat_cli -p \"Review this for security issues: resource aws_s3_bucket data { acl = public-read }\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Download Model Checkpoint\n",
    "\n",
    "Save your trained model before the Kaggle session ends!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zip the model checkpoint\n",
    "!zip -r iac-gpt-model.zip ~/.cache/nanochat/checkpoints/ ~/.cache/nanochat/tokenizer/\n",
    "print(\"Download from Kaggle Output tab\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
