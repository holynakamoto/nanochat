{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# IaC-GPT GPU Training (Kaggle/Colab)\n\nTrain nanochat IaC-GPT on GPU accelerators.\n\n**Setup:**\n1. Kaggle: Settings ‚Üí Accelerator ‚Üí GPU T4 x2 or P100 x2\n2. Colab: Runtime ‚Üí Change runtime type ‚Üí GPU (T4)\n3. Run all cells\n\n**GPU Support:**\n- Kaggle: GPU T4 x2 (2x 16GB VRAM, 30 hrs/week free) or P100 x2 (2x 16GB)\n- Colab: T4 (16GB VRAM, limited hours)\n- Native bfloat16 on Ampere+ GPUs (A100, H100)\n- ~2-4 hours for full d12 training on T4 x2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies using uv (better dependency resolution than pip)\n# Step 1: Install uv\n!curl -LsSf https://astral.sh/uv/install.sh | sh\n!source $HOME/.cargo/env\n\n# Step 2: Use uv to install all dependencies (GPU version - no torch-xla)\n!~/.cargo/bin/uv pip install --system \\\n    torch \\\n    tiktoken pyarrow filelock rustbpe wandb tabulate regex zstandard pyyaml \\\n    anthropic\n\nprint(\"‚úÖ Installation complete via uv\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Clone nanochat repo (ROBUST FIX - prevents nested directories)\nimport os\nimport subprocess\n\n# Determine root directory\nroot_dir = \"/content\" if os.path.exists(\"/content\") else (\"/kaggle/working\" if os.path.exists(\"/kaggle/working\") else os.path.expanduser(\"~\"))\nos.chdir(root_dir)\nprint(f\"Working from: {os.getcwd()}\")\n\n# Clean up any nested mess\nif os.path.exists(\"nanochat/nanochat\"):\n    print(\"‚ö†Ô∏è  Detected nested directories, removing entire nanochat folder...\")\n    import shutil\n    shutil.rmtree(\"nanochat\")\n    print(\"‚úÖ Cleaned up nested directories\")\n\n# Clone or update\nif os.path.exists(\"nanochat/.git\"):\n    print(\"‚úÖ Updating existing nanochat repo...\")\n    subprocess.run([\"git\", \"-C\", \"nanochat\", \"pull\", \"origin\", \"master\"], check=True)\nelse:\n    print(\"üì• Cloning fresh nanochat repo...\")\n    subprocess.run([\"git\", \"clone\", \"https://github.com/holynakamoto/iacgpt.git\", \"nanochat\"], check=True)\n\n# Change to nanochat directory\nos.chdir(\"nanochat\")\nfinal_path = os.getcwd()\nprint(f\"\\n‚úÖ Repository ready at: {final_path}\")\n\n# Safety check\nif final_path.count(\"nanochat\") > 1:\n    print(\"‚ùå ERROR: Still nested! Path contains 'nanochat' multiple times\")\n    print(\"   Please manually delete the nanochat folder and re-run this cell\")\nelse:\n    print(\"‚úÖ Path is clean (no nesting)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify GPU detection\nimport torch\n\nprint(\"=\" * 70)\nprint(\"GPU DETECTION TEST\")\nprint(\"=\" * 70)\n\nif torch.cuda.is_available():\n    print(f\"‚úÖ CUDA available: {torch.cuda.get_device_name(0)}\")\n    print(f\"   GPU count: {torch.cuda.device_count()}\")\n    print(f\"   CUDA version: {torch.version.cuda}\")\n    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n    \n    # Test tensor operation\n    x = torch.randn(3, 3, device='cuda')\n    y = x @ x.t()\n    print(f\"\\n‚úÖ Test matmul successful: {y.shape}\")\nelse:\n    print(\"‚ö†Ô∏è  No CUDA GPU detected! Training will be VERY slow on CPU.\")\n    print(\"   Change Kaggle accelerator to: GPU T4 x2 or P100\")\n\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## GPU Support Status\n\n‚úÖ **Native GPU/CUDA support built into nanochat!**\n\nThe following files auto-detect and optimize for GPU:\n- `common.py`: Auto-detects CUDA and handles device initialization\n- `scripts/base_train.py`: Uses optimized CUDA kernels and mixed precision\n- `gpt.py`: Flash Attention 2/3 for Ampere+ GPUs, SDPA fallback for older GPUs\n\nNo manual configuration needed - just run the training command!"
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Setup Claude API for automated log analysis (OPTIONAL)\nimport os\nfrom getpass import getpass\n\n# Check if API key is already set\nif not os.environ.get(\"ANTHROPIC_API_KEY\"):\n    try:\n        api_key = getpass(\"Enter your Anthropic API key (or press Enter to skip): \")\n        if api_key:\n            os.environ[\"ANTHROPIC_API_KEY\"] = api_key\n            print(\"‚úÖ API key set! Claude will analyze logs automatically.\")\n        else:\n            print(\"‚ö†Ô∏è  Skipped. Logs won't be auto-analyzed by Claude.\")\n    except:\n        print(\"‚ö†Ô∏è  Could not set API key. Continuing without auto-analysis.\")\nelse:\n    print(\"‚úÖ API key already set from environment.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prepare IaC Training Data\n\n**Expanded corpus: 110+ repos across Terraform, Kubernetes, Ansible, Crossplane, Helm, Docker, Pulumi**\n\n**‚ö° Smart Caching:**\n- **First run**: Scrapes 110+ repos (~15-30 min) ‚Üí creates 8-15 parquet shards\n- **Future runs**: Uses cached Kaggle Dataset (~5 seconds) if available\n\n**To cache data across sessions:**\n1. After first run, click **\"Save Version\"** ‚Üí **\"Save & Run All\"**\n2. In Output sidebar, click **\"‚ãÆ\"** next to `/kaggle/working/iac_training_data` ‚Üí **\"Create Dataset\"**\n3. Name it `iac-training-corpus`, click **\"Create\"**\n4. In future notebooks: **\"Add Input\"** ‚Üí search `iac-training-corpus` ‚Üí attach it\n5. Cell will auto-detect and use cached data!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, glob, subprocess, io, shutil\nfrom contextlib import redirect_stdout, redirect_stderr\n\n# Helper function to capture and analyze logs with Claude\ndef analyze_with_claude(logs, task_name):\n    \"\"\"Send logs to Claude for analysis\"\"\"\n    if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n        return  # Skip if no API key\n    \n    try:\n        import anthropic\n        client = anthropic.Anthropic()\n        \n        print(\"\\n\" + \"=\" * 80)\n        print(f\"ü§ñ Analyzing {task_name} with Claude...\")\n        print(\"=\" * 80)\n        \n        response = client.messages.create(\n            model=\"claude-sonnet-4-5-20250929\",\n            max_tokens=2000,\n            messages=[{\n                \"role\": \"user\",\n                \"content\": f\"\"\"Analyze these logs from IaC data scraping in a Kaggle notebook.\n\nTask: {task_name}\n\nLogs:\n{logs}\n\nProvide:\n1. Summary: What happened (success/failure counts, extracted file counts)\n2. Issues: Any errors or warnings that need attention\n3. Recommendations: How to improve results if only 1 shard was created\n\nBe concise and actionable.\"\"\"\n            }]\n        )\n        \n        print(response.content[0].text)\n        print(\"=\" * 80 + \"\\n\")\n        \n    except Exception as e:\n        print(f\"‚ö†Ô∏è  Claude analysis failed: {e}\\n\")\n\n# Setup directories\nCACHE_DIR = os.path.expanduser(\"~/.cache/nanochat\")\nDATA_DIR = os.path.join(CACHE_DIR, \"iac_data\")\nBASE_DATA = os.path.join(CACHE_DIR, \"base_data\")\nKAGGLE_OUTPUT = \"/kaggle/working/iac_training_data\"\n\n# Check for cached Kaggle Dataset\nprint(\"=\" * 80)\nprint(\"Checking for cached IaC training data...\")\nprint(\"=\" * 80)\n\ncached_data_path = None\n# Look for any attached dataset with parquet files\nfor input_dir in glob.glob(\"/kaggle/input/*\"):\n    parquet_files = glob.glob(f\"{input_dir}/*.parquet\")\n    if parquet_files:\n        cached_data_path = input_dir\n        print(f\"‚úÖ Found cached data: {cached_data_path}\")\n        print(f\"   Contains {len(parquet_files)} parquet shards\")\n        break\n\nif cached_data_path:\n    # Use cached data (fast path)\n    print(\"\\n‚ö° Using cached dataset - skipping 15-30 min scraping!\")\n    \n    # Create symlink to cached data\n    os.makedirs(CACHE_DIR, exist_ok=True)\n    if os.path.islink(BASE_DATA):\n        os.unlink(BASE_DATA)\n    elif os.path.exists(BASE_DATA):\n        shutil.rmtree(BASE_DATA)\n    \n    os.symlink(cached_data_path, BASE_DATA)\n    \n    shard_count = len(glob.glob(f'{BASE_DATA}/*.parquet'))\n    print(f\"‚úÖ Loaded {shard_count} parquet shards from cache\")\n    \n    # Show shard contents\n    print(\"\\nCached data contents:\")\n    for f in sorted(glob.glob(f'{BASE_DATA}/*.parquet')):\n        size_mb = os.path.getsize(f) / (1024 * 1024)\n        print(f\"  {os.path.basename(f):30s} {size_mb:6.2f} MB\")\n    \n    print(\"\\n\" + \"=\" * 80)\n    print(\"‚úÖ Data ready - proceed to Cell 9 to train tokenizer\")\n    print(\"=\" * 80)\n\nelse:\n    # No cache found - run full scraping (slow path)\n    print(\"‚ö†Ô∏è  No cached data found - running full scraping (~15-30 min)\")\n    print(\"üí° After this completes, save as Dataset to avoid re-scraping!\\n\")\n    \n    captured_logs = io.StringIO()\n    \n    print(\"=\" * 80)\n    print(\"Updating nanochat to latest version...\")\n    print(\"=\" * 80)\n    result = subprocess.run([\"git\", \"pull\", \"origin\", \"master\"], cwd=\".\", capture_output=True, text=True)\n    print(result.stdout)\n    captured_logs.write(result.stdout + \"\\n\")\n    if result.stderr:\n        print(result.stderr)\n        captured_logs.write(result.stderr + \"\\n\")\n    \n    # Verify we have the expanded repo list\n    result = subprocess.run([\"grep\", \"-c\", \"terraform-aws-modules\", \"dev/fast_scrape_iac.sh\"], \n                           capture_output=True, text=True)\n    repo_count = int(result.stdout.strip()) if result.returncode == 0 else 0\n    msg = f\"‚úÖ Script has {repo_count} terraform-aws-modules repos\\n\"\n    print(msg)\n    captured_logs.write(msg)\n    \n    if repo_count < 20:\n        msg = \"‚ö†Ô∏è  WARNING: Script may not be updated! Should have 20+ terraform-aws-modules repos\\n\"\n        print(msg)\n        captured_logs.write(msg)\n    else:\n        msg = \"‚úÖ Script is updated with expanded repo list\\n\"\n        print(msg)\n        captured_logs.write(msg)\n    \n    # Scrape 110+ IaC repositories\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Scraping 110+ IaC repositories...\")\n    print(\"This will take ~15-30 minutes\")\n    print(\"=\" * 80)\n    \n    scrape_result = subprocess.run([\"bash\", \"dev/fast_scrape_iac.sh\"], \n                                   input=\"n\\n\", capture_output=True, text=True)\n    print(scrape_result.stdout)\n    captured_logs.write(scrape_result.stdout + \"\\n\")\n    if scrape_result.stderr:\n        print(scrape_result.stderr)\n        captured_logs.write(scrape_result.stderr + \"\\n\")\n    \n    # Convert to training shards\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Converting to parquet shards...\")\n    print(\"=\" * 80)\n    repack_result = subprocess.run([\n        \"python3\", \"dev/repackage_iac_data.py\",\n        \"--input-dir\", \"data/iac_raw_cloned\",\n        \"--output-dir\", DATA_DIR,\n        \"--include-synthetic\", \"--include-docs\"\n    ], capture_output=True, text=True)\n    print(repack_result.stdout)\n    captured_logs.write(repack_result.stdout + \"\\n\")\n    if repack_result.stderr:\n        print(repack_result.stderr)\n        captured_logs.write(repack_result.stderr + \"\\n\")\n    \n    # Link base_data\n    if os.path.islink(BASE_DATA):\n        os.unlink(BASE_DATA)\n    if not os.path.exists(BASE_DATA):\n        os.symlink(DATA_DIR, BASE_DATA)\n    \n    shard_count = len(glob.glob(f'{BASE_DATA}/*.parquet'))\n    msg = f\"\\n{'=' * 80}\\n‚úÖ Data ready: {shard_count} shards\\nLocation: {BASE_DATA}\\n{'=' * 80}\\n\"\n    print(msg)\n    captured_logs.write(msg)\n    \n    # Show shard contents\n    print(\"\\nData directory contents:\")\n    for f in sorted(glob.glob(f'{BASE_DATA}/*.parquet')):\n        size_mb = os.path.getsize(f) / (1024 * 1024)\n        shard_info = f\"  {os.path.basename(f):30s} {size_mb:6.2f} MB\"\n        print(shard_info)\n        captured_logs.write(shard_info + \"\\n\")\n    \n    # Copy to Kaggle output directory for dataset creation\n    print(\"\\n\" + \"=\" * 80)\n    print(\"Copying data to /kaggle/working for dataset creation...\")\n    print(\"=\" * 80)\n    \n    if os.path.exists(KAGGLE_OUTPUT):\n        shutil.rmtree(KAGGLE_OUTPUT)\n    shutil.copytree(BASE_DATA, KAGGLE_OUTPUT)\n    \n    print(f\"‚úÖ Copied {len(os.listdir(KAGGLE_OUTPUT))} files to {KAGGLE_OUTPUT}\")\n    print(\"\\nüìã To cache this data for future runs:\")\n    print(\"   1. Click 'Save Version' ‚Üí 'Save & Run All'\")\n    print(\"   2. In Output sidebar ‚Üí '‚ãÆ' next to iac_training_data ‚Üí 'Create Dataset'\")\n    print(\"   3. Name: iac-training-corpus\")\n    print(\"   4. Future notebooks: Add Input ‚Üí search iac-training-corpus\")\n    print(\"=\" * 80)\n    \n    # Analyze logs with Claude\n    analyze_with_claude(captured_logs.getvalue(), \"IaC Data Scraping\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE tokenizer\n",
    "!python3 -m scripts.tok_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Train on GPU (CUDA)\n\nUses PyTorch DDP for multi-GPU training (T4 x2, P100 x2)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# GPU training command (optimized for T4 x2)\nMODEL_DEPTH = 12\nBATCH_SIZE = 4  # T4 has 16GB VRAM, conservative batch size\nWINDOW_PATTERN = \"L\"  # Full attention (T4 has good memory bandwidth)\n\n# Single GPU command\nsingle_gpu_cmd = f\"\"\"python3 scripts/base_train.py \\\n    --depth={MODEL_DEPTH} \\\n    --device-batch-size={BATCH_SIZE} \\\n    --window-pattern={WINDOW_PATTERN} \\\n    --target-param-data-ratio=8 \\\n    --run=iac-gpt-kaggle \\\n    --model-tag=iac-gpt-gpu-d{MODEL_DEPTH} \\\n    --eval-every=100 \\\n    --sample-every=100 \\\n    --save-every=500\"\"\"\n\n# Multi-GPU command (T4 x2, P100 x2)\nmulti_gpu_cmd = f\"\"\"torchrun --nproc_per_node=2 -m scripts.base_train -- \\\n    --depth={MODEL_DEPTH} \\\n    --device-batch-size={BATCH_SIZE} \\\n    --window-pattern={WINDOW_PATTERN} \\\n    --target-param-data-ratio=8 \\\n    --run=iac-gpt-kaggle \\\n    --model-tag=iac-gpt-gpu-d{MODEL_DEPTH} \\\n    --eval-every=100 \\\n    --sample-every=100 \\\n    --save-every=500\"\"\"\n\n# Detect GPU count\nimport torch\ngpu_count = torch.cuda.device_count()\n\nprint(\"=\" * 80)\nprint(f\"GPU Training Command ({gpu_count} GPU{'s' if gpu_count > 1 else ''}):\")\nprint(\"=\" * 80)\n\nif gpu_count > 1:\n    print(f\"\\nMulti-GPU (torchrun):\\n{multi_gpu_cmd}\\n\")\n    print(\"Or run in a new cell:\")\n    print(f\"!{multi_gpu_cmd}\")\nelse:\n    print(f\"\\nSingle GPU:\\n{single_gpu_cmd}\\n\")\n    print(\"Or run in a new cell:\")\n    print(f\"!{single_gpu_cmd}\")\n\nprint(\"\\n‚ö†Ô∏è  IMPORTANT: Run cells 7 and 9 first to prepare data and tokenizer!\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Full Training Pipeline\n\n**IMPORTANT: Run cells in this order:**\n\n1. **Cells 1-3**: Setup (install dependencies, clone repo, verify GPU)\n2. **Cell 5**: (Optional) Enter Anthropic API key for auto-log analysis\n3. **Cell 7**: üî¥ Prepare IaC training data (uses cache if available, otherwise scrapes)\n   - **First run**: ~15-30 min to scrape 110+ repos\n   - **Future runs**: ~5 seconds with cached dataset\n4. **Cell 9**: üî¥ Train BPE tokenizer on IaC data (~2-3 min)\n5. **Cell 11**: Copy the training command and run it\n\n**What each step does:**\n- **Cell 7**: \n  - Checks for cached dataset first (fast path)\n  - If no cache: Clones 110+ repos (Terraform, K8s, Ansible, Crossplane, Helm, Docker, Pulumi) ‚Üí parquet shards\n  - Saves data to `/kaggle/working/iac_training_data/` for dataset creation\n- **Cell 9**: Trains 49K vocab BPE tokenizer on IaC corpus ‚Üí saves to `~/.cache/nanochat/tokenizer/`\n- **Training**: Pretrains d12 model (124M params) on IaC data with Muon optimizer\n\n**Caching workflow (do this after first run):**\n1. **After Cell 7 completes**, click **\"Save Version\"** ‚Üí **\"Save & Run All\"**\n2. In **Output** sidebar (right), find `/kaggle/working/iac_training_data/`\n3. Click **\"‚ãÆ\"** ‚Üí **\"Create Dataset\"**\n4. Name: `iac-training-corpus`, click **\"Create\"**\n5. **In future notebooks**: \n   - Click **\"Add Input\"** (right sidebar)\n   - Search `iac-training-corpus`\n   - Click **\"+\"** to attach\n   - Cell 7 will auto-detect and use it!\n\n**Expected corpus size:**\n- 110+ repos ‚Üí ~100-200MB raw IaC code\n- ~50-100M tokens (after tokenization with compression ratio 3-4x)\n- 8-15 parquet shards for training\n\n**Training time on Kaggle:**\n- **T4 x2**: ~3-4 hours for full training (good for d12)\n- **P100 x2**: ~4-5 hours (older architecture)\n- **A100** (paid): ~1-2 hours (best performance)\n\n**For distributed training on multiple nodes:**\n```bash\n# On each node, run:\ntorchrun --nproc_per_node=2 \\\n    --nnodes=2 --node_rank=<0 or 1> \\\n    --master_addr=<ip> --master_port=29500 \\\n    -m scripts.base_train -- [args...]\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}