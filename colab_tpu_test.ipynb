{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IaC-GPT TPU Training (Colab Prototype)\n",
    "\n",
    "Test nanochat training on Google Colab TPUs before deploying to Kaggle TPU v5e-8.\n",
    "\n",
    "**Setup:**\n",
    "1. Runtime → Change runtime type → TPU (v2-8 or v3-8)\n",
    "2. Run all cells\n",
    "\n",
    "**TPU Specs:**\n",
    "- Colab offers TPU v2-8 (8 cores, 64GB HBM) or v3-8 (8 cores, 128GB HBM)\n",
    "- Native bfloat16 support\n",
    "- ~10x faster than dual T4 GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies in order to avoid conflicts\n# Step 1: Upgrade Google Cloud libraries to versions compatible with protobuf 4.x+\n!pip install -q --upgrade \"google-api-core>=2.27.0\" \"google-cloud-storage>=3.9.0\"\n\n# Step 2: Install torch + torch-xla for TPU\n!pip install -q torch==2.9.0\n!pip install -q torch-xla==2.9.0\n\n# Step 3: Install cloud-tpu-client (needs protobuf 4.x+)\n!pip install -q cloud-tpu-client\n\n# Step 4: Install remaining dependencies\n!pip install -q tiktoken pyarrow filelock rustbpe wandb tabulate regex zstandard pyyaml"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone nanochat repo\n",
    "!git clone https://github.com/holynakamoto/iacgpt.git nanochat 2>/dev/null || \\\n",
    "    (cd nanochat && git pull origin master)\n",
    "%cd nanochat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify TPU detection\n",
    "import torch\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"TPU DETECTION TEST\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "device = xm.xla_device()\n",
    "print(f\"TPU device: {device}\")\n",
    "print(f\"Number of TPU cores: {xm.xrt_world_size()}\")\n",
    "\n",
    "# Test tensor operation\n",
    "x = torch.randn(3, 3).to(device)\n",
    "y = x @ x.t()\n",
    "print(f\"\\nTest matmul: {y.shape}\")\n",
    "print(f\"Device type: {y.device}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add TPU Support to common.py\n",
    "\n",
    "We need to patch the device detection to recognize TPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Patch common.py to add TPU support\n",
    "import os\n",
    "\n",
    "tpu_patch = '''\n",
    "def autodetect_device_type():\n",
    "    # Check for TPU first (Colab, Kaggle)\n",
    "    try:\n",
    "        import torch_xla.core.xla_model as xm\n",
    "        device = xm.xla_device()\n",
    "        device_type = \"xla\"\n",
    "        print0(f\"Autodetected device type: {device_type} (TPU with {xm.xrt_world_size()} cores)\")\n",
    "        return device_type\n",
    "    except ImportError:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print0(f\"TPU detection failed: {e}\")\n",
    "    \n",
    "    # Fallback to CUDA/MPS/CPU\n",
    "    if torch.cuda.is_available():\n",
    "        device_type = \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        device_type = \"mps\"\n",
    "    else:\n",
    "        device_type = \"cpu\"\n",
    "    print0(f\"Autodetected device type: {device_type}\")\n",
    "    return device_type\n",
    "'''\n",
    "\n",
    "# Read current common.py\n",
    "with open('common.py', 'r') as f:\n",
    "    content = f.read()\n",
    "\n",
    "# Replace autodetect_device_type function\n",
    "import re\n",
    "pattern = r'def autodetect_device_type\\(\\):.*?return device_type'\n",
    "content = re.sub(pattern, tpu_patch.strip(), content, flags=re.DOTALL)\n",
    "\n",
    "with open('common.py', 'w') as f:\n",
    "    f.write(content)\n",
    "\n",
    "print(\"✅ Patched common.py with TPU support\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare IaC Training Data\n",
    "\n",
    "Same data pipeline as GPU training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, glob, subprocess\n",
    "\n",
    "CACHE_DIR = os.path.expanduser(\"~/.cache/nanochat\")\n",
    "DATA_DIR = os.path.join(CACHE_DIR, \"iac_data\")\n",
    "BASE_DATA = os.path.join(CACHE_DIR, \"base_data\")\n",
    "\n",
    "# Quick data prep (minimal dataset for testing)\n",
    "print(\"Preparing minimal IaC dataset for TPU testing...\")\n",
    "subprocess.run([\"bash\", \"dev/fast_scrape_iac.sh\"], input=b\"n\", check=True)\n",
    "\n",
    "# Convert to training shards\n",
    "subprocess.run([\n",
    "    \"python3\", \"dev/repackage_iac_data.py\",\n",
    "    \"--input-dir\", \"data/iac_raw_cloned\",\n",
    "    \"--output-dir\", DATA_DIR,\n",
    "    \"--include-synthetic\", \"--include-docs\"\n",
    "], check=True)\n",
    "\n",
    "# Link base_data\n",
    "if os.path.islink(BASE_DATA):\n",
    "    os.unlink(BASE_DATA)\n",
    "os.symlink(DATA_DIR, BASE_DATA)\n",
    "\n",
    "print(f\"✅ Data ready: {len(glob.glob(f'{BASE_DATA}/*.parquet'))} shards\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE tokenizer\n",
    "!python3 -m scripts.tok_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on TPU (XLA)\n",
    "\n",
    "Use torch_xla's distributed launcher instead of torchrun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TPU training command\n",
    "# Note: This requires modifications to base_train.py to use XLA\n",
    "\n",
    "MODEL_DEPTH = 12\n",
    "BATCH_SIZE = 4  # Can be larger on TPU due to 64-128GB HBM\n",
    "NUM_CORES = 8   # TPU v2-8 or v3-8\n",
    "\n",
    "cmd = f\"\"\"python3 -m torch_xla.distributed.xla_dist \\\n",
    "    --tpu-vm --num-cores={NUM_CORES} \\\n",
    "    scripts/base_train.py -- \\\n",
    "    --depth={MODEL_DEPTH} \\\n",
    "    --device-batch-size={BATCH_SIZE} \\\n",
    "    --window-pattern=L \\\n",
    "    --target-param-data-ratio=8 \\\n",
    "    --run=dummy \\\n",
    "    --model-tag=iac-gpt-tpu-d{MODEL_DEPTH} \\\n",
    "    --eval-every=100 \\\n",
    "    --sample-every=100 \\\n",
    "    --save-every=100\"\"\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TPU TRAINING COMMAND:\")\n",
    "print(cmd)\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n⚠️  Note: base_train.py needs XLA modifications first!\")\n",
    "print(\"Next step: Patch base_train.py for XLA compatibility\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "To complete TPU support:\n",
    "\n",
    "1. **Modify base_train.py:**\n",
    "   - Replace `torch.distributed` with `torch_xla.distributed`\n",
    "   - Use `xm.optimizer_step(optimizer)` instead of `optimizer.step()`\n",
    "   - Use `xm.all_reduce()` for gradient synchronization\n",
    "\n",
    "2. **Modify engine.py:**\n",
    "   - Add XLA-specific compilation flags\n",
    "   - Use `xm.mark_step()` after backward pass\n",
    "\n",
    "3. **Test on Colab TPU v2-8**\n",
    "\n",
    "4. **Port to Kaggle for TPU v5e-8**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}