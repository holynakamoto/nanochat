{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# IaC-GPT TPU Training (Colab/Kaggle Prototype)\n\nTest nanochat training on TPUs before production deployment.\n\n**Setup:**\n1. Runtime ‚Üí Change runtime type ‚Üí TPU\n2. Run all cells\n\n**TPU Support:**\n- Colab: TPU v2-8 (8 cores, 64GB HBM) or v3-8 (8 cores, 128GB HBM)\n- Kaggle: TPU v5e-1 (1 core, 16GB HBM) or v5e-8 (8 cores, 128GB HBM)\n- Native bfloat16 support\n- ~5-10x faster than T4 GPUs for transformer training"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Install dependencies using uv (better dependency resolution than pip)\n# Step 1: Install uv\n!curl -LsSf https://astral.sh/uv/install.sh | sh\n!source $HOME/.cargo/env\n\n# Step 2: Use uv to install all dependencies (handles conflicts automatically)\n!~/.cargo/bin/uv pip install --system \\\n    torch==2.9.0 \\\n    torch-xla==2.9.0 \\\n    cloud-tpu-client \\\n    \"google-api-core>=2.27.0\" \\\n    \"google-cloud-storage>=3.9.0\" \\\n    \"protobuf>=4.25.2,<6.0\" \\\n    tiktoken pyarrow filelock rustbpe wandb tabulate regex zstandard pyyaml\n\nprint(\"‚úÖ Installation complete via uv\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone nanochat repo\n",
    "!git clone https://github.com/holynakamoto/iacgpt.git nanochat 2>/dev/null || \\\n",
    "    (cd nanochat && git pull origin master)\n",
    "%cd nanochat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify TPU detection (updated for torch-xla 2.9.0 API)\nimport torch\nimport torch_xla\nimport torch_xla.core.xla_model as xm\nimport torch_xla.runtime as xr\n\nprint(\"=\" * 70)\nprint(\"TPU DETECTION TEST\")\nprint(\"=\" * 70)\n\n# Use new torch_xla 2.9.0 API\ndevice = torch_xla.device()\nprint(f\"TPU device: {device}\")\nprint(f\"Number of TPU cores: {xr.world_size()}\")\nprint(f\"Local ordinal: {xr.local_ordinal()}\")\nprint(f\"Global ordinal: {xr.global_ordinal()}\")\n\n# Test tensor operation\nx = torch.randn(3, 3, device=device)\ny = x @ x.t()\nprint(f\"\\nTest matmul: {y.shape}\")\nprint(f\"Device type: {y.device}\")\nprint(\"=\" * 70)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## TPU Support Status\n\n‚úÖ **TPU/XLA support is now built into nanochat!**\n\nThe following files have native TPU support:\n- `common.py`: Auto-detects TPU and handles device initialization\n- `scripts/base_train.py`: Uses XLA-specific optimizer and synchronization\n\nNo manual patching needed - just run the training command below!"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verify nanochat has TPU support (should auto-detect)\nimport sys\nsys.path.insert(0, '.')\n\nfrom common import autodetect_device_type\n\ndevice_type = autodetect_device_type()\nprint(f\"\\n‚úÖ Nanochat detected device type: {device_type}\")\n\nif device_type != \"xla\":\n    print(\"‚ö†Ô∏è  Warning: Expected device_type='xla' but got '{device_type}'\")\n    print(\"Make sure you selected TPU runtime and installed torch-xla correctly\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Prepare IaC Training Data\n\n**Expanded corpus: 110+ repos across Terraform, Kubernetes, Ansible, Crossplane, Helm, Docker, Pulumi**\n\nThis will take ~15-30 minutes and produce ~100-200MB of IaC code ‚Üí 8-15 parquet shards."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os, glob, subprocess\n\n# Pull latest nanochat code (includes expanded 110+ repo list)\nprint(\"=\" * 80)\nprint(\"Updating nanochat to latest version...\")\nprint(\"=\" * 80)\nsubprocess.run([\"git\", \"pull\", \"origin\", \"master\"], cwd=\".\", check=True)\nprint(\"\\n‚úÖ Updated to latest version with 110+ IaC repos\\n\")\n\nCACHE_DIR = os.path.expanduser(\"~/.cache/nanochat\")\nDATA_DIR = os.path.join(CACHE_DIR, \"iac_data\")\nBASE_DATA = os.path.join(CACHE_DIR, \"base_data\")\n\n# Scrape 110+ IaC repositories (expanded corpus)\nprint(\"=\" * 80)\nprint(\"Scraping 110+ IaC repositories...\")\nprint(\"This will take ~15-30 minutes\")\nprint(\"=\" * 80)\nsubprocess.run([\"bash\", \"dev/fast_scrape_iac.sh\"], input=b\"n\", check=True)\n\n# Convert to training shards\nprint(\"\\n\" + \"=\" * 80)\nprint(\"Converting to parquet shards...\")\nprint(\"=\" * 80)\nsubprocess.run([\n    \"python3\", \"dev/repackage_iac_data.py\",\n    \"--input-dir\", \"data/iac_raw_cloned\",\n    \"--output-dir\", DATA_DIR,\n    \"--include-synthetic\", \"--include-docs\"\n], check=True)\n\n# Link base_data\nif os.path.islink(BASE_DATA):\n    os.unlink(BASE_DATA)\nos.symlink(DATA_DIR, BASE_DATA)\n\nshard_count = len(glob.glob(f'{BASE_DATA}/*.parquet'))\nprint(\"\\n\" + \"=\" * 80)\nprint(f\"‚úÖ Data ready: {shard_count} shards\")\nprint(f\"Location: {BASE_DATA}\")\nprint(\"=\" * 80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train BPE tokenizer\n",
    "!python3 -m scripts.tok_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train on TPU (XLA)\n",
    "\n",
    "Use torch_xla's distributed launcher instead of torchrun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TPU training command (v5e-1 single-core)\nMODEL_DEPTH = 12\nBATCH_SIZE = 8  # v5e-1 has 16GB HBM, can handle larger batches\n\n# Training command (device type will auto-detect as 'xla')\ncmd = f\"\"\"python3 scripts/base_train.py \\\n    --depth={MODEL_DEPTH} \\\n    --device-batch-size={BATCH_SIZE} \\\n    --window-pattern=L \\\n    --target-param-data-ratio=8 \\\n    --run=dummy \\\n    --model-tag=iac-gpt-tpu-d{MODEL_DEPTH} \\\n    --eval-every=100 \\\n    --sample-every=100 \\\n    --save-every=100\"\"\"\n\nprint(\"=\" * 80)\nprint(\"TPU v5e-1 TRAINING COMMAND:\")\nprint(cmd)\nprint(\"=\" * 80)\nprint(\"\\n‚ö†Ô∏è  IMPORTANT: Run cells 7 and 9 first to prepare data and tokenizer!\")\nprint(\"Then uncomment and run the training command below:\\n\")\nprint(f\"!{cmd}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Full Training Pipeline\n\n**IMPORTANT: Run cells in this order:**\n\n1. **Cells 1-5**: Setup (install dependencies, clone repo, verify TPU)\n2. **Cell 7**: üî¥ Scrape 110+ IaC repos + create training shards (~15-30 min, expect 8-15 shards)\n3. **Cell 9**: üî¥ Train BPE tokenizer on IaC data (~2-3 min)\n4. **Cell 11**: Copy the training command and run it\n\n**What each step does:**\n- **Cell 7**: Clones 110+ repos (Terraform, K8s, Ansible, Crossplane, Helm, Docker, Pulumi) ‚Üí parquet shards at `~/.cache/nanochat/base_data/`\n- **Cell 9**: Trains 49K vocab BPE tokenizer on IaC corpus ‚Üí saves to `~/.cache/nanochat/tokenizer/`\n- **Training**: Pretrains d12 model (124M params) on IaC data with Muon optimizer\n\n**Expected corpus size:**\n- 110+ repos ‚Üí ~100-200MB raw IaC code\n- ~50-100M tokens (after tokenization with compression ratio 3-4x)\n- 8-15 parquet shards for training\n\n**For multi-core TPU (v2-8, v3-8, v5e-8):**\n```bash\npython3 -m torch_xla.distributed.xla_dist \\\n    --tpu-vm --num-cores=8 \\\n    scripts/base_train.py -- [args...]\n```"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}